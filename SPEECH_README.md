# Azure AI Speech Service - Personal Voice Tutorial

This README walks you through the steps to create and use a personal voice with Azure AI Speech Service.

## Prerequisites

1. An Azure account with an active subscription
2. Azure AI Speech resource created in the Azure portal
3. Speech Service subscription key and region
4. Python 3.7 or later
5. Azure Cognitive Services Speech SDK for Python (`pip install azure-cognitiveservices-speech`)

## Step 1: Set up environment variables

Set your Azure Speech Service subscription key and region as environment variables:

```bash
export AZURE_SPEECH_KEY="your_speech_key_here"
export AZURE_SPEECH_REGION="your_speech_region_here"
```

## Step 2: Create a personal voice project

Use the Azure Speech Service API to create a personal voice project. This can only be done through the API (not via Speech Studio).

```bash
curl -v -X PUT \
  -H "Ocp-Apim-Subscription-Key: $AZURE_SPEECH_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "description": "My Personal Voice Project",
    "kind": "PersonalVoice"
  }' \
  "https://$AZURE_SPEECH_REGION.api.cognitive.microsoft.com/customvoice/projects/MyPersonalVoiceProject?api-version=2024-02-01-preview"
```

Note the `ProjectId` from the response, which will be needed in subsequent steps.

## Step 3: Add user consent to the project

Before creating a personal voice, you need to add user consent:

```bash
curl -v -X PUT \
  -H "Ocp-Apim-Subscription-Key: $AZURE_SPEECH_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "projectId": "MyPersonalVoiceProject",
    "consentDocuments": {
      "consentStatement": "I [your name] am aware that [person's name] is creating a synthetic voice that sounds like me, and I consent to [person's name] creating this synthetic voice."
    }
  }' \
  "https://$AZURE_SPEECH_REGION.api.cognitive.microsoft.com/customvoice/consents/MyConsentId?api-version=2024-02-01-preview"
```

Note the `consentId` from the response.

## Step 4: Create a personal voice and get a speaker profile ID

You'll need audio recordings of the speaker's voice to create a personal voice. The audio should be:
- Clean human voice samples between 5-90 seconds
- Formats: mp3 or wav
- Sampling rates: 16 kHz, 24 kHz, 44.1 kHz, or 48 kHz

Upload the audio files:

```bash
curl -v -X POST \
  -H "Ocp-Apim-Subscription-Key: $AZURE_SPEECH_KEY" \
  -F 'projectId="MyPersonalVoiceProject"' \
  -F 'consentId="MyConsentId"' \
  -F 'audiodata=@"path/to/voice_sample1.wav"' \
  -F 'audiodata=@"path/to/voice_sample2.wav"' \
  "https://$AZURE_SPEECH_REGION.api.cognitive.microsoft.com/customvoice/personalvoices/MyPersonalVoiceId?api-version=2024-02-01-preview"
```

From the response, note the `speakerProfileId` which will be used in the text-to-speech application.

## Step 5: Use the personal voice in text-to-speech

Update the `text_to_speech.py` script with your `speakerProfileId` and run it:

```python
# Uncomment and update this line in the script
speaker_profile_id = "your_speaker_profile_id_here"  # From Step 4
personal_voice_text_to_speech("This is my personal voice generated by Azure AI Speech Service.", 
                            speaker_profile_id,
                            "personal_voice_output.wav")
```

Run the script:

```bash
python text_to_speech.py
```

## Understanding SSML for Personal Voice

Personal voices require using Speech Synthesis Markup Language (SSML) with the `speakerProfileId` attribute:

```xml
<speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xmlns:mstts='http://www.w3.org/2001/mstts' xml:lang='en-US'>
    <voice name='DragonLatestNeural'> 
        <mstts:ttsembedding speakerProfileId='your_speaker_profile_id'> 
            Your text goes here.
        </mstts:ttsembedding> 
    </voice> 
</speak>
```

- `DragonLatestNeural` provides superior voice cloning similarity
- `PhoenixLatestNeural` offers more accurate pronunciation with lower latency

## Supported Features

Personal voice supports:
- Speaking in 91+ languages across 100+ locales
- Automatic language detection at the sentence level
- Switching languages using the `<lang xml:lang>` element

## References

- [Create a project for personal voice](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/personal-voice-create-project)
- [Add user consent to the project](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/personal-voice-create-consent)
- [Get a speaker profile ID](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/personal-voice-create-voice)
- [Use personal voice in your application](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/personal-voice-how-to-use)
- [Speech SDK documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech)